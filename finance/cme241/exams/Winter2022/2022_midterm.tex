\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pseudocode}
\usepackage{etoolbox}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{MnSymbol,wasysym}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}

% Environment for problem solution
\printanswers
% \noprintanswers

% Use environment \begin{solution}...\end{solution} if solution has multiple lines



\begin{document}
\begin{center}
{\large {\bf Stanford CME 241 (Winter 2022) - Midterm Exam}}
\end{center}
{\large{\bf Instructions:}}
\begin{itemize}
\item We trust that you will follow \href{https://communitystandards.stanford.edu/policies-and-guidance/honor-code}{The Stanford Honor Code}.
\item You have about 48 hours to work on this test, and need to submit your answers by 9 am PT on Wednesday 2/9. However, the estimated amount of work for this test is about 6 hours, depending on your proficiency in typesetting mathematical notations and in writing code. There are 3 problems (with subproblems), with a total of 25 points.
\item Include all of your writing, math formulas, code, graphs etc. into a single answers-document. Be sure to write your name and SUNET ID in your answers-document. You can do your work in LaTeX or Markdown or Jupyter or any other typesetting option, which should finally be converted to the single answers-document PDF to be submitted. Note: Code can be included in LaTeX using the {\em lstlisting} environment, and graphs can be included using {\em includegraphics}.
\item Submit your answers-document on Gradescope. Please ensure you have access to Gradescope before starting the exam so there is ample time to correct the problem. The Gradescope assignments for this midterm are \href{https://www.gradescope.com/courses/245396}{here}.
\item Do not share or upload your code or written work publicly, and make sure to include the code files as part of your submission on Gradescope
\item Submit a PDF of the written portion of your work to the \verb|Midterm| Assignment on Gradescope, submit the code files to the \verb|Midterm - Code| assignment on Gradescope
\end{itemize}

 \newpage{}
{\large{\bf Problems:}}
\begin{questions}

\question{\bf  Question 1: Gaps Value Iteration Optimization.} In class we introduced the idea of the gaps-based asynchronous Value Iteration algorithm. In this question, we will implement this algorithm for Finite MDPs and see how it can effect convergence time for an environment with sparse rewards compared to an environment with dense rewards.

The algorithm works as follows
\begin{enumerate}
    \item Initialize the value function to zero for all states: $v[s] = 0\ \forall s \in \mathcal{N}$
    \item Calculate the gaps for each state: $g[s] = |v[s] - \max_a \mathcal{R}(s,a) + \gamma \sum_{s'} \mathcal{P}(s,a,s') \cdot v(s')|$
    \item While there is some gap that exceeds a threshold
    \begin{itemize}
        \item Select the state with the  largest gap: $s_{max} = \arg\max_{s \in \mathcal{N}} g[s]$
        \item Update the value function for $s_{max}$: $v[s_{max}] = \max_a \mathcal{R}(s_{max},a) + \gamma \sum_{s'}\mathcal{P}(s_{max},a,s') \cdot v(s')$
        \item Update the gap for $s_{max}$: $g[s_{max}] = 0$
        \item Update the gaps for each state which depends directly on $s_{max}$:
        \[\forall s \in D(s_{max}):\ g[s] = |v[s] - \max_a \mathcal{R}(s,a) + \gamma \sum_{s'} \mathcal{P}(s,a,s') \cdot v(s')|\]
        \[D(s) = \{s' \in S|\ \mathcal{P}(s', a, s) > 0 \text{ for some action } a\}\]
    \end{itemize}
    \item Return v
\end{enumerate}


Please pull the latest copy of the $\href{https://github.com/TikhonJelvis/RL-book/tree/master/}{RL-Book}$ repository (i.e. run \verb|$ git pull| on the master branch); we have provided the code skeleton for the problem in the  $\href{https://github.com/TikhonJelvis/RL-book/tree/master/rl/midterm_22}{midterm\_2022}$  folder. We have included two utility files which you should not edit, but which you may want to read through to familiarize yourself with the code. We have also included a jupyter notebook which has the skeleton for the code you should implement. 

\begin{enumerate}
    \item {\bf \href{https://github.com/TikhonJelvis/RL-book/tree/master/rl/midterm\_2022/grid\_maze.py/}{grid\_maze.py}}
    \begin{itemize}
        \item This file contains the class definitions for the GridMaze MDPs
        \item You should note the difference in the reward functions between \verb|GridMazeMDP_Dense| and \verb|GridMazeMDP_Sparse|
        \item You should not edit the code in this file
    \end{itemize}
    \item {\bf \href{https://github.com/TikhonJelvis/RL-book/tree/master/rl/midterm\_2022/priority\_q.py}{priority\_q.py}}
    \begin{itemize}
        \item This file contains the implementation of a the priority queue you should use to store the gaps
        \item You should not edit the code in this file
        \item You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations where $n$ is the number of elements in the queue
        \begin{enumerate}
            \item isEmpty(self): check if the queue is empty -- Runtime: $O(1)$
            \item contains(self, element): check if the queue contains an element -- Runtime: $O(1)$
            \item peek(self): peek at the highest priority element in the queue -- Runtime: $O(1)$
            \item pop(self): remove and return the highest priority element in the queue -- Runtime: $O(\log(n))$
            \item insert(self, element, priority): insert an element into the queue with given priority -- Runtime: $O(\log(n))$
            \item update(self, element, new\_priority): update the priority of an element in the queue -- Runtime: $O(\log(n))$
            \item delete(self, element): delete an element from the queue -- Runtime: $O(\log(n))$
        \end{enumerate}
        \item We have included examples of the usage of the PriorityQueue class in the implementation file.
    \end{itemize}
    \item {\bf \href{https://github.com/TikhonJelvis/RL-book/tree/master/rl/midterm\_2022/midterm-22-P1-Skeleton.ipynb}{midterm-22-P1-Skeleton.ipynb}}
    \begin{itemize}
        \item This file is a jupyter notebook which has the code-skeleton which you should complete
        \item You {\bf should}  edit the code in this file {\em where indicated}
        \item to run the notebook, 
        \begin{enumerate}
            \item cd into the RL-Book repository: \\
                \verb|$ cd {your repo location}/RL-Book| 
            \item activate the environment corresponding to the class:\\
                \verb|$ source {your venv location}/bin/activate| 
            \item run jupyter: \\
                \verb|$ jupyter notebook| 
            \item on a web-browser navigate to \verb|localhost:8888| (or whatever port number is indicated by the output from the previous command), then navigate to the \verb|midterm_2022| folder 
            \item if you have trouble with running this, email Sven: \verb|svenl@stanford.edu| directly
        \end{enumerate}
    \end{itemize}
\end{enumerate}


\bigskip

\textbf{Question 1 Problems}

\begin{enumerate}
    \item {\bf 2 pts:} Implement the invert\_transition\_mapping function, this should give you the states whose value function directly depends on another state's value function. i.e. this function maps $s \xrightarrow{} I(s)$ where $I(s) = \{s' \in S|\ P(s',a,s) >0\ \text{for some } a\}$. This will help you update the gaps in the second step of the algorithm
    
    \item {\bf 4 pts:} Implement "gaps\_value\_iteration" and the wrapper function "gaps\_value\_iteration\_result" using the algorithm above. ({\em Hint:} ensure that you are not adding unnecessary elements to the queue). You may wish to look at the implementation of the corresponding functions in \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py}{dynamic\_programming.py}
    
    Test the time for convergence of your algorithm for the SparseGridMaze MDP with $\gamma = 0.9$ and the DenseGridMaze MDP with $\gamma = 1$ and compare it to the convergence time for standard value iteration. Also ensure that the value functions produced by your code matches the value function produced by the dynamic programming library. {\bf Note} we have provided code for you to carry out this step.
    
    \item {\bf 1 pts:}  Explain the difference between the convergence time for the four configurations (Gaps / Standard value iteration on the Dense / Sparse GridMaze).
\end{enumerate}


\newpage{}
\question{\bf Question 2}
Assume that we are in discrete time, and imagine that at every time $t$ you are allowed to allocate your wealth into two assets $(A_1, A_2)$ which pay you money back at time $t+1$. For each dollar invested in $A_1$ at time $t$ you receive $X_{t+1}$ dollars at time $t+1$; for each dollar invested in $A_2$ at time $t$ you receive $Y_{t+1}$ dollars at time $t+1$. The joint probability of $(X_{t+1}, Y_{t+1})$ is invariant in time and follows the following (infinite) Probability Mass Function:

\[(X_{t+1}, Y_{t+1}) \sim \begin{cases}(1,0) &\text{with probability } \frac{1}{2}\\
(0, 2^i) &\text{with probability } \frac{1}{2^{i+1}} \quad \text{for } i\in \mathbb{Z}^+ \end{cases}\]

You have a investment horizon of $T$ time-steps and your goal is to maximize the utility of your wealth at the end of $T$ time-steps by dynamically re-balancing your portfolio at each time-step. Assume that you have $\log$ utility ($U(w) = \log(w)$). You must invest all of your wealth at each time across the two assets, so you can represent your portfolio at any time as 
\[\begin{cases} W_t(1-\pi_t) &\text{invested in } A_1\\ 
                    W_t \pi_t &\text{invested in }  A_2
\end{cases}\]

You begin at time $t=0$ with some initial wealth $W_0$, and the only changes in your wealth result from changes in the value of your portfolio, i.e.

\[W_{t+1} = W_t((1-\pi_t)X_{t+1} + \pi_t Y_{t+1})\]

\begin{enumerate}
    \item {\bf 3 points:} Consider the case where $T=1$, what is the optimal portfolio choice, $\pi$ which maximizes the expected utility of wealth after one time-step, $E[U(W_{1})]$ where 
    \[W_{1} =  W_0*((1-\pi)X_{1} + \pi Y_{1})\]

\begin{solution}

\begin{align*}
    E[U(W_{1})] &= E[U(W_0*((1-\pi) X_{t+1} + (1-\pi)Y_{t+1}))] \\
    &= \frac{1}{2}U(W_0 (1-\pi)) + \sum_{i=1}^{\infty}\frac{1}{2^{i+1}}U(2^i \pi W_0)\\
    &= \sum_{i=1}^n \frac{\log(W_0) + \log(\pi) + \log(2^i)}{2^{i+1}} +  \log(W_0*(1-\pi))/2\\
\end{align*}

Now we differentiate w.r.t. $\pi$ and equate to zero

\begin{align*}
    0 &= \sum_{i=1}^n \frac{\frac{1}{\pi}}{2^{i+1}} +  \frac{1}{2(1-\pi)}\\
\end{align*}

This implies that 

\[\sum_{i=1}^{\infty} \frac{1}{2^{i}} \frac{1}{\pi} = \frac{1}{1-\pi}\]

\[\frac{1}{\pi} = \frac{1}{1-\pi}\]

so $\pi^* = \frac{1}{2}$

\end{solution}

    \item {\bf 6 points:} Prove that the optimal policy when you have to invest over $T > 1$ periods is the same as the optimal policy for $T=1$ (i.e.) $\pi_t^*=\pi^*$. 
    
\begin{solution}

Allow $V_{t}(W)$ to be the value function corresponding to this MDP with wealth level $W$, if $t=T$ then $V_{t}(W)=U(W)$. Now introduce our policy $\pi_t$, we have the recurrence 

\[V_{t}^{\pi}(W) = E[V_{t+1}^{\pi}(W_{t+1})| \pi_t]\]

Now, note that if $V_{t+1}^{\pi}(W_{t+1})$ is of the form $\log(W_{t+1}\alpha_{t+1})$ for some $\alpha_{t+1}$, then the optimal policy $\pi_t^*$ simply maximizes $\mathbb{E}[\log(W_{t+1})]$, thus our our policy from part $a$ is optimal. We will show that if 
\[V_{t+1}^{\pi}(W_{t+1}) = \log(W_{t+1}\alpha_{t+1})\]

then 

\[V_{t}^{\pi}(W_{t+1}) = \log(W_{t+1}\alpha_{t})\]

\begin{align*}
    V_{t}^{\pi}(W_{t}) &= E[V_{t+1}^{\pi}(W_{t+1})| \pi_t] \\
    &= \frac{1}{2} V_{t+1}^{\pi}((1- \pi_t) W_t) + \frac{1}{2}\sum_{i=1}^{\infty}\frac{1}{2^i}V_{t+1}(\pi_tW_t 2^i)\\
    &= \frac{1}{2} \log((1-\pi_t) W_t\alpha_{t+1}) + \frac{1}{2}\sum_{i=1}^{\infty}\frac{1}{2^i}\log(\pi_tW_t 2^i \alpha_{t+1})\\
    &= \sum_{i=1}^{\infty}\log(W_t)\frac{1}{2^i} + K\\
    &= \log(W_t\alpha_t)\\
\end{align*}

So, we have shown that if the value function corresponding to $V_{t+1}$ is logarithmic in $W_{t+1}$, then the value function corresponding to $V_{t}$ is also logarithmic in $W_t$, moreover if the value function $V_{t+1}$ is logarithmic in $W_{t+1}$ then the optimal policy at time $t$ is the same as the one step policy.


\end{solution}

\end{enumerate}


\newpage{}
\question{\bf Question 3: Dice Rolling}

Consider the following dice game. You start with $N$ $K$-sided dice on the table, and no dice in your hand. The values on the dice faces are $\{1, 2, ..., K\}$. While you have dice remaining on the table, the game proceeds as follows:

\begin{enumerate}
    \item roll all the dice on the table
    \item select a nonempty subset of the dice on the table to move to your hand, the dice you move to your hand keep the value which they were just rolled. For example, if your hand is $\{1,3\}$ and you roll $(2,2,3,4)$ and you decide to move the dice with $3 \& 4$ to your hand, you will now have $\{1,3,3,4\}$ in your hand.
\end{enumerate}

The game ends when you have no dice on the table left to roll. Your score for the game is then calculated as the sum of the values of dice in your hand if you have at least $C$ 1's in your hand, and zero otherwise. For example, for $N=K=4$ and $C=2$, the score corresponding to a hand containing (1, 3, 1, 4) would be 9 while the score corresponding to a hand containing (4, 1, 3, 4) would be 0.
    
Your goal is to maximize your score at the end of the game.

\begin{itemize}
\item {\bf 4 points:} With proper mathematical notation, model this as a Finite MDP specifying the states, actions, rewards, state-transition probabilities and discount factor.
\item {\bf 5 points:} Implement this MDP in python. If you wish, you may use the code in the git repo that you forked at the start of the course (eg: \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/markov\_decision\_process.py#L232}{FiniteMarkovDecisionProcess}), but if you prefer, you can implement it from scratch or using code you have written for the course previously (whichever is more convenient for you). You should implement this for the general case, specifically your MDP implementation should take as parameters $N,K,C$.

For $N=6, K=4, C=1$, use the  \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py}{dynamic\_programming.py} library (or your own code if you chose not to implement it within the class library) to solve for the optimal value function, and present the following values, 
\begin{enumerate}
    \item The expected score of the game playing optimally, calculate this using your code, {\em not } analytically
    \item The optimal action when rolling $\{1,2,2,3,3,4\}$ on the first roll
\end{enumerate}

\end{itemize}

\begin{solution}

The state space is all pairs
\[(H, B) \in Z_{0,+}^K \times Z_{_+}^K,\quad \text{such that } \sum_{i} H_i + B_i = N\]

The nonterminal states are all states with $B\neq 0$

The actions from state $(H, B)$ are 
\[A \in Z_{0,+}^K \quad \text{such that } \sum_{i} A_ \geq 1,\ A \preceq B \]

The transitions are 

\[P((H, B), A, (H', B')) = \begin{cases} \frac{\prod_{i=1}^K {(\sum_{j=i}^{K} B'_j) \choose B'_j}}{K^{\sum_{B'_i}}} &\text{if } H' = H + A,\ \sum_{i}B_i' + A' = \sum_i B_i \\ 0 &\text{otherwise}\end{cases}\]

\end{solution}




\end{questions}

\end{document}
